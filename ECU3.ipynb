{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Claudita22/nuevo_repositorio/blob/main/ECU3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Claudia Guadalupe Quezada LÃ³pez\n"
      ],
      "metadata": {
        "id": "HXuBUG3GIghE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zegqTl54ud_J"
      },
      "outputs": [],
      "source": [
        "\n",
        "!uv pip install -q --system numba-cuda==0.4.0\n",
        "!pip install pynvjitlink-cu12\n",
        "import numpy as np\n",
        "from numba import cuda\n",
        "import time\n",
        "import os\n",
        "from numba import config\n",
        "import numba\n",
        "config.CUDA_ENABLE_PYNVJITLINK=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikVODLI5vErk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numba import cuda\n",
        "import math\n",
        "import time\n",
        "\n",
        "@cuda.jit\n",
        "def vector_add_kernel(a, b, c):\n",
        "  \"\"\"\n",
        "  Each thread computes one element: c[i] = a[i] + b[i]\n",
        "  \"\"\"\n",
        "  #compute global thread index\n",
        "  idx = cuda.grid(1)\n",
        "\n",
        "  #Boundary check\n",
        "  if idx < c.size:\n",
        "    c[idx] = a[idx] + b[idx]\n",
        "\n",
        "\n",
        "def main():\n",
        "  N_large = 10_000_000\n",
        "  a = np.random.randn(N_large).astype(np.float32)\n",
        "  b = np.random.randn(N_large).astype(np.float32)\n",
        "  c = np.zeros(N_large, dtype=np.float32)\n",
        "\n",
        "  d_a = cuda.to_device(a)\n",
        "  d_b = cuda.to_device(b)\n",
        "  d_c = cuda.to_device(c)\n",
        "\n",
        "  threads_per_block = 256\n",
        "  brocks_per_grid = math.ceil(N_large / threads_per_block)\n",
        "\n",
        "  #Warmup\n",
        "  vector_add_kernel[brocks_per_grid, threads_per_block](d_a, d_b, d_c)\n",
        "  cuda.synchronize()\n",
        "\n",
        "  #GPU timing\n",
        "  start = time.time()\n",
        "  vector_add_kernel[brocks_per_grid, threads_per_block](d_a, d_b, d_c)\n",
        "  cuda.synchronize()\n",
        "  gpu_time = (time.time() - start) * 1000\n",
        "\n",
        "  result = d_c.copy_to_host()\n",
        "\n",
        "  #CPU timing\n",
        "  cpu_start = time.time()\n",
        "  expected = a + b # Fixed: Changed 'bin' to 'b'\n",
        "  cpu_time = (time.time() - cpu_start)*1000\n",
        "\n",
        "  print(f\"GPU kernel time: {gpu_time:.3f} ms\")\n",
        "  print(f\"CPU Numpy time: {cpu_time:.3f} ms\")\n",
        "  print(f\"Speedup: {cpu_time/gpu_time:.3f}x\")\n",
        "  print(\"Correct:\", np.allclose(result, expected))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LumUiXd8AA2R"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numba import cuda\n",
        "import math\n",
        "import time\n",
        "\n",
        "@cuda.jit\n",
        "def dummy_compute_kernel(a, b, c):\n",
        "  \"\"\"\n",
        "  Each thread computes one element: c[i] = sqrt(a[i]^2 + b[i]^2)\n",
        "  \"\"\"\n",
        "  #compute global thread index\n",
        "  idx = cuda.grid(1)\n",
        "\n",
        "  #Boundary check\n",
        "  if idx < c.size:\n",
        "    c[idx] = math.sqrt(a[idx]**2 + b[idx]**2)\n",
        "\n",
        "\n",
        "def main():\n",
        "  N_large = 10_000_000\n",
        "  a = np.random.randn(N_large).astype(np.float32)\n",
        "  b = np.random.randn(N_large).astype(np.float32)\n",
        "  c = np.zeros(N_large, dtype=np.float32)\n",
        "\n",
        "  d_a = cuda.to_device(a)\n",
        "  d_b = cuda.to_device(b)\n",
        "  d_c = cuda.to_device(c)\n",
        "\n",
        "  threads_per_block = 256\n",
        "  brocks_per_grid = math.ceil(N_large / threads_per_block)\n",
        "\n",
        "  #Warmup\n",
        "  dummy_compute_kernel[brocks_per_grid, threads_per_block](d_a, d_b, d_c)\n",
        "  cuda.synchronize()\n",
        "\n",
        "  #GPU timing\n",
        "  start = time.time()\n",
        "  dummy_compute_kernel[brocks_per_grid, threads_per_block](d_a, d_b, d_c)\n",
        "  cuda.synchronize()\n",
        "  gpu_time = (time.time() - start) * 1000\n",
        "\n",
        "  result = d_c.copy_to_host()\n",
        "\n",
        "  #CPU timing\n",
        "  cpu_start = time.time()\n",
        "  expected = np.sqrt(a**2 + b**2) # Fixed: Changed 'bin' to 'b'\n",
        "  cpu_time = (time.time() - cpu_start)*1000\n",
        "\n",
        "  print(f\"GPU kernel time: {gpu_time:.3f} ms\")\n",
        "  print(f\"CPU Numpy time: {cpu_time:.3f} ms\")\n",
        "  print(f\"Speedup: {cpu_time/gpu_time:.3f}x\")\n",
        "  print(\"Correct:\", np.allclose(result, expected))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KstbjSJ5HQG9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numba import cuda\n",
        "import math\n",
        "import time\n",
        "\n",
        "@cuda.jit\n",
        "def matrix_scale_kernel(mat, scalar, out):\n",
        "  \"\"\"\n",
        "  Scalar every element: out[row, col] = mat[row, col] * scalar\n",
        "  \"\"\"\n",
        "  #compute global thread index\n",
        "  row, col = cuda.grid(2)\n",
        "\n",
        "  #Boundary check\n",
        "  if row < out.shape[0] and col < out.shape[1]:\n",
        "    out[row, col] = mat[row, col] * scalar\n",
        "\n",
        "def main():\n",
        "  rows_large, cols_large = 4096, 4096\n",
        "  mat = np.random.randn(rows_large, cols_large).astype(np.float32)\n",
        "  out = np.zeros_like(mat)\n",
        "  scalar = 2.5\n",
        "  d_mat = cuda.to_device(mat)\n",
        "  d_out = cuda.to_device(out)\n",
        "\n",
        "  threads_per_block = (16, 16)\n",
        "  blocks_per_grid_x = math.ceil(rows_large / threads_per_block[0])\n",
        "  blocks_per_grid_y = math.ceil(cols_large / threads_per_block[1])\n",
        "  blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)\n",
        "\n",
        "  #Warmup\n",
        "  matrix_scale_kernel[blocks_per_grid, threads_per_block](d_mat, scalar, d_out)\n",
        "  cuda.synchronize()\n",
        "\n",
        "  #GPU timing\n",
        "  start = time.time()\n",
        "  matrix_scale_kernel[blocks_per_grid, threads_per_block](d_mat, scalar, d_out)\n",
        "  cuda.synchronize()\n",
        "  gpu_time = (time.time() - start) * 1000\n",
        "\n",
        "  result = d_out.copy_to_host()\n",
        "\n",
        "  #CPU timing\n",
        "  cpu_start = time.time()\n",
        "  expected = mat*scalar\n",
        "  cpu_time = (time.time() - cpu_start)*1000\n",
        "\n",
        "  print(f\"GPU kernel time: {gpu_time:.3f} ms\")\n",
        "  print(f\"CPU Numpy time: {cpu_time:.3f} ms\")\n",
        "  print(f\"Speedup: {cpu_time/gpu_time:.3f}x\")\n",
        "  print(\"Correct:\", np.allclose(result, expected))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2f4D5jCLPDJ"
      },
      "outputs": [],
      "source": [
        "#Matrix multiplications\n",
        "import numpy as np\n",
        "from numba import cuda\n",
        "import math\n",
        "import time\n",
        "\n",
        "@cuda.jit\n",
        "def matmul_naive_kernel(A, B, C):\n",
        "  \"\"\"\n",
        "  Naive matrix multiply: C= A@B\n",
        "  Each thread computes one element of C\n",
        "  All reads from A and B go to global memory (slow)\n",
        "  A: (M, K)\n",
        "  B: (K, N)\n",
        "  C: (M, N)\n",
        "  \"\"\"\n",
        "  row, col = cuda.grid(2)\n",
        "\n",
        "  M, K = A.shape\n",
        "  K2, N = B.shape\n",
        "\n",
        "  #Boundary check\n",
        "  if row < M and col < N:\n",
        "    total = 0.0\n",
        "    for k in range(K):\n",
        "      total += A[row, k] * B[k, col]\n",
        "    C[row, col] = total\n",
        "\n",
        "def main():\n",
        "  M, K, N = 1000, 1000, 1000\n",
        "  A = np.random.randn(M, K).astype(np.float32)\n",
        "  B = np.random.randn(K, N).astype(np.float32)\n",
        "  C = np.zeros((M, N), dtype=np.float32)\n",
        "\n",
        "  threads_per_block = (32, 32)\n",
        "  d_A = cuda.to_device(A)\n",
        "  d_B = cuda.to_device(B)\n",
        "  d_C = cuda.to_device(C)\n",
        "\n",
        "  # Correct calculation for blocks_per_grid to ensure integer values\n",
        "  blocks_per_grid_x = math.ceil(M / threads_per_block[0])\n",
        "  blocks_per_grid_y = math.ceil(N / threads_per_block[1])\n",
        "  blocks_per_grid = (int(blocks_per_grid_x), int(blocks_per_grid_y))\n",
        "\n",
        "  #Warmup\n",
        "  matmul_naive_kernel[blocks_per_grid, threads_per_block](d_A, d_B, d_C)\n",
        "  cuda.synchronize()\n",
        "\n",
        "  #GPU timing\n",
        "  start = time.time()\n",
        "  matmul_naive_kernel[blocks_per_grid, threads_per_block](d_A, d_B, d_C)\n",
        "  cuda.synchronize()\n",
        "  gpu_time = (time.time() - start) * 1000\n",
        "\n",
        "  c_gpu = d_C.copy_to_host()\n",
        "\n",
        "  #CPU timing\n",
        "  cpu_start = time.time()\n",
        "  expected = A@B\n",
        "  cpu_time = (time.time() - cpu_start)*1000\n",
        "\n",
        "  print(f\"GPU kernel time: {gpu_time:.3f} ms\")\n",
        "  print(f\"CPU Numpy time: {cpu_time:.3f} ms\")\n",
        "  print(f\"Speedup: {cpu_time/gpu_time:.3f}x\")\n",
        "  print(\"Correct: {np.allclose(c_gpu, expected, atol=1e-3)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yA4700D_bmgi"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_O-2sJ0Rkt9"
      },
      "outputs": [],
      "source": [
        "#Aplicacion con filtro de sobel\n",
        "import numpy as np\n",
        "import numba.cuda as cuda\n",
        "import time\n",
        "import urllib.request\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "\n",
        "@cuda.jit\n",
        "def sobel_kernel(img, out):\n",
        "  \"\"\"\n",
        "  Apply sobel edge detection - each thread processes one pixel\n",
        "  \"\"\"\n",
        "  row, col = cuda.grid(2)\n",
        "  H, W = img.shape\n",
        "\n",
        "  #Boundary check\n",
        "  if 0 < row < H-1 and 0 < col < W-1:\n",
        "    #Horizontal gradient (Gx)\n",
        "    gx = (-img[row-1, col-1] + img[row-1, col+1]+\n",
        "          -2*img[row, col-1] + 2*img[row,col+1]+\n",
        "          -img[row+1, col-1] + img[row+1, col+1])\n",
        "\n",
        "    #vertical gradient (Gy)\n",
        "    gy = (-img[row-1, col-1] - 2*img[row-1, col] - img[row-1, col+1]+\n",
        "          img[row+1, col-1] + 2*img[row+1, col] + img[row+1, col+1])\n",
        "\n",
        "    #edge magnitude\n",
        "    out[row, col] = (gx**2 + gy**2)**0.5\n",
        "\n",
        "\n",
        "def sobel_opencv(img):\n",
        "  gx = cv2.Sobel(img, cv2.CV_32F, 1, 0, ksize=3)\n",
        "  gy = cv2.Sobel(img, cv2.CV_32F, 0, 1, ksize=3)\n",
        "  return np.sqrt(gx**2 + gy**2)\n",
        "\n",
        "\n",
        "#load 4K image from internet\n",
        "urllib.request.urlretrieve(\"https://picsum.photos/3840/2160\", \"image.jpg\")\n",
        "img = Image.open(\"image.jpg\").convert(\"L\")\n",
        "img = np.asarray(img, dtype=np.float32)\n",
        "\n",
        "H, W = img.shape\n",
        "print(f\"Image: {W}x{H} ({W*H:,} pixels)\")\n",
        "\n",
        "d_img = cuda.to_device(img)\n",
        "d_out = cuda.to_device(np.zeros_like(img))\n",
        "\n",
        "threads = (32, 32)\n",
        "blocks = (W + 15) // 16, (H + 15) // 16\n",
        "\n",
        "print(f\"Grid: {blocks} blocks x {threads} threads\")\n",
        "\n",
        "#Warmup\n",
        "sobel_kernel[blocks, threads](d_img, d_out)\n",
        "cuda.synchronize()\n",
        "\n",
        "#GPU timing\n",
        "start = time.time()\n",
        "sobel_kernel[blocks, threads](d_img, d_out)\n",
        "cuda.synchronize()\n",
        "gpu_time = (time.time() - start) * 1000\n",
        "\n",
        "out_gpu = d_out.copy_to_host()\n",
        "\n",
        "#CPU timing\n",
        "cpu_start = time.time()\n",
        "out_cpu = sobel_opencv(img)\n",
        "cpu_time = (time.time() - cpu_start)*1000\n",
        "\n",
        "#results\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Results\")\n",
        "print(\"=\"*50)\n",
        "print(f\"GPU kernel time: {gpu_time:.3f} ms\")\n",
        "print(f\"CPU Numpy time: {cpu_time:.3f} ms\")\n",
        "print(f\"Speedup: {cpu_time/gpu_time:.3f}x\")\n",
        "print(f\"Correct: {np.allclose(out_gpu, out_cpu, atol=1e-3)}\")\n",
        "\n",
        "# Resize for display\n",
        "H, W = img.shape\n",
        "target_w = 256\n",
        "target_h = int(target_w * H / W)\n",
        "\n",
        "def resize_for_plot(array):\n",
        "    normalized = (array / array.max() * 255).astype(np.uint8)\n",
        "    return np.array(Image.fromarray(normalized).resize((target_w, target_h), Image.LANCZOS))\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(resize_for_plot(img), cmap='gray')\n",
        "plt.title('Original Image')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(resize_for_plot(out_gpu), cmap='gray')\n",
        "plt.title('GPU Sobel Edges')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(resize_for_plot(out_cpu), cmap='gray')\n",
        "plt.title('OpenCV CPU Sobel Edges')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ]
}